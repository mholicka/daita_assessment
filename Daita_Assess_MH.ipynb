{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be672780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed items\n",
    "from lightfm_dataset_helper.lightfm_dataset_helper import DatasetHelper\n",
    "import pandas as pd\n",
    "from lightfm.data import Dataset\n",
    "from scipy.stats import zscore\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "NUM_THREADS= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2095a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data, ensure cols are stripped for matching\n",
    "ratings_csv = 'data/ratings.csv'\n",
    "features_csv = 'data/features.csv'\n",
    "\n",
    "ratings_df = pd.read_csv(ratings_csv)\n",
    "features_df = pd.read_csv(features_csv)\n",
    "\n",
    "\n",
    "features_df.rename(columns=lambda x: x.strip()) \n",
    "ratings_df.rename(columns=lambda x: x.strip()) \n",
    "\n",
    "# Incase there are some missing product_ids, drop them\n",
    "ratings_df.dropna(axis=0, subset=(['product_id']),inplace=True)\n",
    "features_df.dropna(axis=0, subset=(['product_id']),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c37324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   7268.000\n",
      "mean      12.837\n",
      "std       37.240\n",
      "min        1.000\n",
      "25%        2.000\n",
      "50%        3.000\n",
      "75%        8.000\n",
      "max      510.000\n",
      "Name: user_id, dtype: float64\n",
      "count         4396.000\n",
      "mean      40687371.145\n",
      "std       99156092.804\n",
      "min           8392.000\n",
      "25%        1653224.000\n",
      "50%        7338804.000\n",
      "75%       32118282.000\n",
      "max     1373392760.000\n",
      "Name: num_ratings, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# We need to do some filtering to ensure that the data is useable\n",
    "# let's examine the data \n",
    "\n",
    "print(ratings_df['user_id'].value_counts().describe())\n",
    "print(features_df['num_ratings'].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30424687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rating Filter - Before :4396  After :4341 \n",
      "Features - Before :4341  Before :3369 \n",
      "Users - Before :93296  After :89128 \n"
     ]
    }
   ],
   "source": [
    "# Seems like the majoity for the users have at least 3 reviews and products at least  1.5m reviews, so let's set the thresolds there\n",
    "min_reviews = 1500000 \n",
    "min_usr_reviews = 3\n",
    "\n",
    "# we don't want features if they have an avg rating of 0\n",
    "before_filter = features_df.shape[0]\n",
    "features_df= features_df[features_df['avg_ratings'] !=0]\n",
    "after_filter = features_df.shape[0]\n",
    "\n",
    "print(f\"Average Rating Filter - Before :{before_filter}  After :{after_filter} \")\n",
    "\n",
    "#filtering based on the data analysis\n",
    "before_filter = features_df.shape[0]\n",
    "features_df = features_df[features_df['num_ratings']>=min_reviews]\n",
    "after_filter = features_df.shape[0]\n",
    "\n",
    "print(f\"Features - Before :{before_filter}  Before :{after_filter} \")\n",
    "\n",
    "before_filter = ratings_df.shape[0]\n",
    "filtered_usr=(ratings_df['user_id'].value_counts()>=min_usr_reviews)\n",
    "filtered_usr= filtered_usr[filtered_usr].index.tolist()\n",
    "ratings_df = ratings_df[(ratings_df['user_id'].isin(filtered_usr))] \n",
    "after_filter = ratings_df.shape[0]\n",
    "print(f\"Users - Before :{before_filter}  After :{after_filter} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0dfd078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization needs to occur in a few places: product avg ratings,num_ratings and reducing the user bias\n",
    "\n",
    "# Z-score normalization for product avg ratings by product id\n",
    "features_df['prod_z']=features_df.groupby('product_id')['avg_ratings'].transform(lambda x : zscore(x))\n",
    "\n",
    "# log normalize num_ratings and divide by max\n",
    "# since lightfm normalized rows, we wish to reduce massive numbers such as num_ratings\n",
    "features_df['log_norm_ratings']=np.log(features_df['num_ratings'])/ features_df.groupby('product_id')['num_ratings'].transform(np.max)\n",
    "\n",
    "# log1p seems to be a standard price normalizer based on research, so let's normalize that\n",
    "features_df['logPrice'] = np.log1p(features_df['price'])\n",
    "\n",
    "#we normalized the 3 columns, so they can be dropped\n",
    "features_df = features_df.drop(columns=['avg_ratings','num_ratings','price'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5e6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the interaction matrix, we need to take into account user bias in the ratings per user.\n",
    "# there are a few approaches, such as subtracting the average user rating from the particular rating, gaussian distribution, etc.\n",
    "# subtracting the averager user rating from the rating is quick and dirty in this case.\n",
    "\n",
    "ratings_df['user_avg'] = ratings_df.groupby(['user_id','product_id'])['rating'].transform('mean')\n",
    "\n",
    "merged_ratings_items = ratings_df.join(features_df[['product_id','log_norm_ratings','prod_z']].set_index('product_id'), on='product_id',how='inner')\n",
    "\n",
    "merged_ratings_items['post_bias'] = merged_ratings_items[\"rating\"] - merged_ratings_items[\"user_avg\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1564f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can construct the user dataframe \n",
    "user_df = ratings_df[['user_id','rating','user_avg']]\n",
    "user_df['bias'] = ratings_df['rating'] - ratings_df['user_avg']\n",
    "user_df['avg_bias'] = user_df.groupby('user_id')['bias'].transform('mean')\n",
    "\n",
    "\n",
    "\n",
    "# there is no need to keep duplicates\n",
    "user_df = user_df[['user_id','avg_bias']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b461c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's construct this lightfm with the interactions, users, and products\n",
    "item_features_cols = list(features_df)\n",
    "\n",
    "items_col = 'product_id'\n",
    "user_col = 'user_id'\n",
    "ratings_col = 'post_bias'\n",
    "\n",
    "# intereraction_df\n",
    "interaction_df = merged_ratings_items[['user_id','product_id',ratings_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d0e4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the dataset helper from pypi, which automates the lightfm creation, we make the model.\n",
    "helper_instance = DatasetHelper(\n",
    "    users_dataframe = user_df,\n",
    "    items_dataframe = features_df,\n",
    "    interactions_dataframe = interaction_df,\n",
    "    item_id_column=items_col,\n",
    "    user_features_columns=['avg_bias'],\n",
    "    items_feature_columns=item_features_cols,\n",
    "    user_id_column =user_col, \n",
    "    interaction_column=ratings_col,\n",
    "    clean_unknown_interactions=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e971c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make the model with the wrap loss, and adagrad learning schedule as we have a low number of epochs\n",
    "from lightfm import LightFM\n",
    "helper_instance.routine()\n",
    "model = LightFM(no_components= 80, loss=\"warp\",item_alpha= 1e-7,user_alpha=1e-7,learning_rate = 0.02,learning_schedule='adagrad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f46a5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm.cross_validation import random_train_test_split\n",
    "import numpy as np\n",
    "# we need to split into test and train, so we set the seed and use 20% for teesting \n",
    "\n",
    "train,test = random_train_test_split(helper_instance.interactions, test_percentage=0.2, random_state=np.random.RandomState(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01cbf709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [02:10<00:00, 13.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# we fit the model \n",
    "cf=model.fit(\n",
    "    interactions = train,\n",
    "    item_features =helper_instance.item_features_list,\n",
    "    verbose=True,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    num_threads=NUM_THREADS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53e5f951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborative filtering train AUC: 0.85382855\n",
      "Collaborative filtering test AUC: 0.8982744\n"
     ]
    }
   ],
   "source": [
    "# Import the evaluation routines\n",
    "from lightfm.evaluation import auc_score\n",
    "\n",
    "\n",
    "# Compute and print the AUC score\n",
    "train_auc = auc_score(cf, train, item_features=helper_instance.item_features_list,num_threads=NUM_THREADS).mean()\n",
    "print('Collaborative filtering train AUC: %s' % train_auc)\n",
    "test_auc = auc_score(cf, test, item_features=helper_instance.item_features_list,num_threads=NUM_THREADS).mean()\n",
    "print('Collaborative filtering test AUC: %s' % test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6614c807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [02:11<00:00, 13.12s/it]\n"
     ]
    }
   ],
   "source": [
    "#We make the hybrid model\n",
    "\n",
    "\n",
    "hybrid_model = model.fit(train,\n",
    "                item_features=helper_instance.item_features_list,\n",
    "                epochs=NUM_EPOCHS,\n",
    "                num_threads=NUM_THREADS,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "371861af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid training set AUC: 0.84924823\n"
     ]
    }
   ],
   "source": [
    "#AUC of the hybrid model\n",
    "train_auc_h = auc_score(hybrid_model,\n",
    "                      train,\n",
    "                      item_features=helper_instance.item_features_list,\n",
    "                      num_threads=NUM_THREADS).mean()\n",
    "\n",
    "print('Hybrid training set AUC: %s' % train_auc_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1816f502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid testing set AUC: 0.89644706\n"
     ]
    }
   ],
   "source": [
    "test_auc_h = auc_score(hybrid_model,\n",
    "                      test_interactions=test,\n",
    "                      item_features=helper_instance.item_features_list,\n",
    "                      num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid testing set AUC: %s' % test_auc_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb7dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "293aed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline\n",
      "        Train  Test\n",
      "CF      0.888 0.347\n",
      "Hybrid  0.860 0.703\n",
      "---------\n",
      "Improved\n",
      "        Train  Test\n",
      "CF      0.854 0.898\n",
      "Hybrid  0.849 0.896\n"
     ]
    }
   ],
   "source": [
    "#visual score representation\n",
    "score_dict = {\n",
    "    \"CF\":{\n",
    "        \"Train\":train_auc,\n",
    "        \"Test\":test_auc\n",
    "    },\n",
    "    \"Hybrid\":{\n",
    "        \"Train\":train_auc_h,\n",
    "        \"Test\":test_auc_h\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "baseline = {\n",
    "    \"CF\":{\n",
    "        \"Train\":0.887519,\n",
    "        \"Test\":0.34728\n",
    "    },\n",
    "    \"Hybrid\":{\n",
    "        \"Train\":0.86049,\n",
    "        \"Test\":0.703039\n",
    "    }\n",
    "}\n",
    "\n",
    "score_df = pd.DataFrame.from_dict(score_dict).T\n",
    "base_df = pd.DataFrame.from_dict(baseline).T\n",
    "\n",
    "print (\"Baseline\")\n",
    "print(base_df)\n",
    "print ('---------')\n",
    "print (\"Improved\")\n",
    "print(score_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f94eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future improvements\n",
    "\n",
    "# 1. scikit hyperparameter optimization [sklearn model_selection.RandomizedSearchCV]\n",
    "# 2. Context of the fields is important for the weighting, such as male/female specific for clothing. Having the fields as just numbers really doesn't help all that much in creating the weights\n",
    "# 3. Increasing Epochs. my macbook was not happy with me when I tried to do 80 epochs, but the more epochs, the better the fit. 10 seemed to be used quite often in my research, so I picked that. \n",
    "#4 There are many rating normalizations from my reserach, and it would have been great if I could have tested them all( main one would have been the beta distribution).\n",
    "#5. Data cleaning could have been much more scientific instead of quantiles with a buffer, but the data seems to be much more improved with these settings\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
